{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting a Boltz Serving App\n",
    "\n",
    "[Boltz](https://github.com/jwohlwend/boltz) is an open-source biomolecular structure prediction model with performance on par with Alphafold3. This notebook details a simple boltz prediction task as well as how to serve a prediction endpoint using FastAPI and Union Serving.\n",
    "\n",
    "## Overview\n",
    "- Define a [remote object](https://docs.union.ai/byoc/user-guide/development-cycle/union-remote/) to interact with the Union cluster\n",
    "- Materialize an Artifact representing the [model on Huggingface](https://huggingface.co/boltz-community/boltz-1)\n",
    "- Create an [ImageSpec](https://docs.union.ai/byoc/user-guide/development-cycle/image-spec#imagespec) definition for use throughout\n",
    "- Create a simple prediction workflow using [Actors](https://docs.union.ai/byoc/user-guide/core-concepts/actors/#actors)\n",
    "- Define a FastAPI serving endpoint\n",
    "- Deploy the app via Union [Serving](https://docs.union.ai/byoc/user-guide/core-concepts/serving/#serving)\n",
    "\n",
    "## Setup\n",
    "- Install the `union` package\n",
    "- Create a config file via `union create login` and make it available at the environment variable below\n",
    "\n",
    "### UnionRemote\n",
    "\n",
    "The following cell will refer to your config file and create a UnionRemote object that's used throughout the rest of the notebook. This object allows you to register entities, trigger executions, and retrieve outputs in a programmatic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from union import UnionRemote, ImageSpec, ActorEnvironment, FlyteFile, FlyteDirectory, workflow, Resources, Artifact\n",
    "from flytekit.configuration import Config\n",
    "\n",
    "os.environ[\"UNION_CONFIG\"] = \"~/.union/config_serving.yaml\"\n",
    "\n",
    "remote = UnionRemote(config=Config.auto(config_file=os.path.expanduser(os.environ[\"UNION_CONFIG\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Model from Huggingface\n",
    "\n",
    "UnionRemote has a convenience function for caching models from Huggingface as Union Artifacts. You'll need to create an API token on HF and then upload it via `union create secret --name HF_TOKEN`. You'll also have to create an admin key via `union create api-key admin --name UNION_API_KEY`.\n",
    "\n",
    "This will run a workflow that fetches the model and emits an Artifact. You can view the execution in the console, as well as the model in the Artifacts tab once the workflow has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from union.remote import HuggingFaceModelInfo\n",
    "info = HuggingFaceModelInfo(repo=\"boltz-community/boltz-1\")\n",
    "\n",
    "cache_exec = remote._create_model_from_hf(\n",
    "    info=info, \n",
    "    hf_token_key=\"HF_TOKEN\", \n",
    "    union_api_key=\"UNION_API_KEY\",\n",
    ")\n",
    "\n",
    "cache_exec = cache_exec.wait(poll_interval=2)\n",
    "cache_exec.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ImageSpec Definition\n",
    "\n",
    "ImageSpec is an easy and flexible way of defining the images you'll be using throughout your workflow and in your apps. A number of options are built in for PyPI packages, conda packages, etc. \n",
    "\n",
    "We define a number of PyPI packages as well as the `build-essential` APT bundle for Boltz. Finally, we install Boltz via an arbitrary RUN command.\n",
    "\n",
    "Of note here is the use of the `union` builder. This will ship the ImageSpec definition off to a hosted builder in your Union cluster. This unburdens your local machine from having to build and push an image yourself, resulting in faster iteration cycles. Moreover, the remote builder uses performance enhancements like layer caching and PyPI proxying to speed up builds even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ImageSpec(\n",
    "    name=\"boltz\",\n",
    "    packages=[\n",
    "        \"union\",\n",
    "        \"flytekit==1.15\",\n",
    "        \"union-runtime==0.1.11\",\n",
    "        \"fastapi==0.115.11\",\n",
    "        \"pydantic==2.10.6\",\n",
    "        \"uvicorn==0.34.0\",\n",
    "        \"python-multipart==0.0.20\",\n",
    "    ],\n",
    "    apt_packages=[\"build-essential\"],\n",
    "    builder=\"union\",\n",
    "    commands=[\"pip install boltz==0.4.1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Workflow\n",
    "\n",
    "Actors are a powerful primitve offering substantial performance improvements as well as unlocking certain capabilities not possible with regular tasks. By using a warm pod capable of accepting multiple task submissions, the overhead of pod scheduling and cleanup are removed. This results in faster iterations between tasks as well as enabling substantial improvements during large parallel executions.\n",
    "\n",
    "We first define an ActorEnvironment using many of the same parameters we're accustomed to for regular tasks. Additionally, we define a replica count and a time-to-live to control parallelism capacity as well as how long to persist between task submissions. Once defined, the actor environment can be used in exactly the same way as the usual `@task` decorator.\n",
    "\n",
    "The workflow itself requires no special treatment regarding actor tasks vs regular tasks. Finally, we call the workflow using `remote.execute`, pass in the input, and await a response. Once the workflow is submitted, head over to the console to watch the actor environment get provisioned and process the prediction!\n",
    "\n",
    "Once the execution succeeds, the actor pod will remain active for the specified 10 minutes. Try changing something in the task itself and run the cell again. Everything will execute and return much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorEnvironment(\n",
    "    name=\"boltz-actor\",\n",
    "    replica_count=1,\n",
    "    ttl_seconds=600,\n",
    "    requests=Resources(\n",
    "        cpu=\"2\",\n",
    "        mem=\"10Gi\",\n",
    "        gpu=\"1\",\n",
    "    ),\n",
    "    container_image=image,\n",
    ")\n",
    "\n",
    "@actor.task\n",
    "def simple_predict(input: FlyteFile) -> FlyteDirectory:\n",
    "    input.download()\n",
    "    out = \"/tmp/boltz_out\"\n",
    "    os.makedirs(out, exist_ok=True)\n",
    "    subprocess.run([\"boltz\", \"predict\", input.path, \"--out_dir\", out, \"--use_msa_server\"])\n",
    "    return FlyteDirectory(path=out)\n",
    "\n",
    "@workflow\n",
    "def act_wf(input: FlyteFile) -> FlyteDirectory:\n",
    "    return simple_predict(input=input)\n",
    "\n",
    "execution = remote.execute(\n",
    "    entity=act_wf, \n",
    "    inputs={\"input\": \"prot_no_msa.yaml\"}, \n",
    "    wait=True\n",
    ")\n",
    "output = execution.outputs\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPI App\n",
    "\n",
    "Here, we initialize our FastAPI application, which will serve as the foundation for our API endpoints.\n",
    "\n",
    "First, we implement some convenience functions: `package_outputs` and the asynchronous `generate_response`. The former is fairly self-explanatory, however the latter manages the execution of the Boltz process, yielding empty bytes during processing to maintain the connection. By implementing this as an asynchronous generator, we ensure our web server remains responsive during potentially long-running Boltz computations.\n",
    "\n",
    "The heart of our implementation is the `/predict/` endpoint, which we define using FastAPI's decorator pattern. This endpoint accepts YAML input sequences and optional configuration parameters, optional MSA (Multiple Sequence Alignment) files, and additional CLI options.\n",
    "\n",
    "Next, we construct and execute the Boltz command with appropriate parameters, including any custom options provided by the client. We've implemented flexibility here - if an MSA file is provided, we use it directly; otherwise, we instruct Boltz to use the `mmseqs2` MSA server for sequence alignments.\n",
    "We're careful to implement robust error handling throughout our application, capturing and returning meaningful error messages if something goes wrong.\n",
    "\n",
    "Finally, the results are streamed back to the client using FastAPI's StreamingResponse, which efficiently delivers the compressed output while setting appropriate headers to prompt the client to handle it as a downloadable file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile boltz_fastapi.py\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import asyncio\n",
    "import tempfile\n",
    "import traceback\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "from click.testing import CliRunner\n",
    "from boltz.main import predict\n",
    "from fastapi import FastAPI, File, UploadFile, Form, BackgroundTasks\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def package_outputs(output_dir: str) -> bytes:\n",
    "    import io\n",
    "    import tarfile\n",
    "\n",
    "    tar_buffer = io.BytesIO()\n",
    "    parent_dir = Path(output_dir).parent\n",
    "\n",
    "    cur_dir = os.getcwd()\n",
    "    with tarfile.open(fileobj=tar_buffer, mode=\"w:gz\") as tar:\n",
    "        os.chdir(parent_dir)\n",
    "        try: \n",
    "            tar.add(Path(output_dir).name, arcname=Path(output_dir).name)\n",
    "        finally: \n",
    "            os.chdir(cur_dir)\n",
    "\n",
    "    return tar_buffer.getvalue()\n",
    "\n",
    "async def generate_response(process, out_dir, yaml_path):\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=10.0)\n",
    "                break\n",
    "            except TimeoutError:\n",
    "                yield b\"\"  # Yield null character instead of spaces\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            raise Exception(stderr.decode())\n",
    "\n",
    "        print(stdout.decode())\n",
    "\n",
    "        # Package the output directory\n",
    "        tar_data = package_outputs(f\"{out_dir}/boltz_results_{Path(yaml_path).with_suffix('').name}\")\n",
    "        yield tar_data\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        yield JSONResponse(status_code=500, content={\"error\": str(e)}).body\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict_endpoint(\n",
    "    yaml_file: UploadFile = File(...),\n",
    "    msa_file: Optional[UploadFile] = File(None),\n",
    "    options: Optional[Dict[str, str]] = Form(None)\n",
    "):\n",
    "    yaml_path = f\"/tmp/{yaml_file.filename}\"\n",
    "    with open(yaml_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(yaml_file.file, buffer)\n",
    "\n",
    "    msa_path = f\"/tmp/{msa_file.filename}\"\n",
    "    with open(msa_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(msa_file.file, buffer)\n",
    "\n",
    "    # Create a temporary directory for the output\n",
    "    with tempfile.TemporaryDirectory() as out_dir:\n",
    "        # Call boltz.predict as a CLI tool\n",
    "        try:\n",
    "            print(f\"Running predictions with options: {options} into directory: {out_dir}\")\n",
    "            # Convert options dictionary to key-value pairs\n",
    "            options_list = [f\"--{key}={value}\" for key, value in (options or {}).items()]\n",
    "            if msa_file and os.path.exists(msa_path):\n",
    "                print(f\"MSA file included at {msa_path}\")\n",
    "            else:\n",
    "                options_list.append(\"--use_msa_server\")\n",
    "            command = [\"boltz\", \"predict\", yaml_path, \"--out_dir\", out_dir, \"--cache\", \"/tmp/.boltz_cache\"] + options_list\n",
    "            print(f\"Running command: {' '.join(command)}\")\n",
    "            process = await asyncio.create_subprocess_exec(\n",
    "                *command,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stderr=asyncio.subprocess.PIPE\n",
    "            )\n",
    "            return StreamingResponse(generate_response(process, out_dir, yaml_path), media_type=\"application/gzip\", headers={\"Content-Disposition\": f\"attachment; filename=boltz_results.tar.gz\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return JSONResponse(status_code=500, content={\"error\": str(e)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving\n",
    "\n",
    "Here we define our Union serving configuration, which specifies how our Boltz FastAPI service will be deployed and managed in your cluster.\n",
    "\n",
    "First, we define our model artifact by creating a reference to the Boltz model that we materialized previously. This artifact definition includes important metadata such as the project, domain, name, and version, as well as partition information that describes the model's characteristics.\n",
    "\n",
    "Following that, we define our FastAPI application deployment. The `App` class encapsulates all the specifications needed to run our service in a containerized environment:\n",
    "\n",
    "- We give our application a name (`boltz-fastapi-notebook`) for identification within the Union system.\n",
    "- We specify the same ImageSpec we've used throughout.\n",
    "- We define an input via the Artifact's `query()` method, downloading it and mounting it at the specified path (`/tmp/.boltz_cache`).\n",
    "- We establish resource limits for our application, including CPU, memory, GPU, and storage requirements.\n",
    "- We set the port our application will listen on and specify the above file to include in the deployment.\n",
    "- We define the command line arguments needed to start our FastAPI server using Uvicorn.\n",
    "- We configure environment variables, such as enabling PyTorch MPS fallback for better compatibility.\n",
    "- We set up auto-scaling parameters, including the minimum and maximum number of replicas, the scale-down timing, and the metric that triggers scaling (in this case, request rate).\n",
    "- Finally, we specify the GPU accelerator type we need, which is an NVIDIA L40S in this implementation.\n",
    "\n",
    "After configuring our application, we prepare for deployment by creating an `AppRemote` instance using the same `remote` object we've been using.\n",
    "\n",
    "In the final step, we deploy our Boltz FastAPI application to the Union platform by calling the `deploy` method on our `AppRemote` instance. This initiates the deployment process, which will provision the necessary infrastructure, deploy our container, and make our Boltz service available according to the specifications we've defined. This will create a publicly accessible URL that leverages the same auth and RBAC as the rest of your cluster.\n",
    "\n",
    "This deployment approach allows our Boltz service to automatically scale based on demand, efficiently utilize GPU resources when needed, and maintain high availability with minimum replicas always running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from union import Resources, ImageSpec\n",
    "from union.app import App, ScalingMetric, Input\n",
    "from flytekit.extras.accelerators import GPUAccelerator\n",
    "\n",
    "boltz_model = Artifact(\n",
    "    project=\"flytesnacks\",\n",
    "    domain=\"development\",\n",
    "    name=\"boltz-1\",\n",
    "    version=\"7c1d83b779e4c65ecc37dfdf0c6b2788076f31e1\",\n",
    "    partitions={\n",
    "        \"task\": \"auto\",\n",
    "        \"model_type\": \"custom\",\n",
    "        \"huggingface-source\": \"boltz-community/boltz-1\",\n",
    "        \"format\": \"None\",\n",
    "        \"architecture\": \"custom\",\n",
    "        \"_u_type\": \"model\",\n",
    "    },\n",
    ")\n",
    "\n",
    "boltz_fastapi = App(\n",
    "    name=\"boltz-fastapi-notebook\",\n",
    "    container_image=image,\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"boltz_model\", value=boltz_model.query(), download=True, mount=\"/tmp/.boltz_cache\"\n",
    "        ),\n",
    "    ],\n",
    "    limits=Resources(cpu=\"2\", mem=\"10Gi\", gpu=\"1\", ephemeral_storage=\"50Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"./boltz_fastapi.py\"],\n",
    "    args=[\"uvicorn\", \"boltz_fastapi:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"],\n",
    "    env={\n",
    "        \"PYTORCH_ENABLE_MPS_FALLBACK\": \"1\",\n",
    "    },\n",
    "    min_replicas=1,\n",
    "    max_replicas=3,\n",
    "    scaledown_after=timedelta(minutes=10),\n",
    "    scaling_metric=ScalingMetric.RequestRate(1),\n",
    "    accelerator=GPUAccelerator(\"nvidia-l40s\"),\n",
    ")\n",
    "\n",
    "from union.remote._app_remote import AppRemote\n",
    "\n",
    "app_remote = AppRemote(default_project=\"default\", default_domain=\"development\", union_remote=remote)\n",
    "\n",
    "app_remote.deploy(boltz_fastapi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying It Out\n",
    "\n",
    "Your app should now be fully provisioned and available at an automatically generated endpoint. Since we're using FastAPI, we get swagger docs for free. Head on over to that endpoint and add `/docs/` to the end to pull up the `/predict/` endpoint specification. You can then try it out by passing in `prot.yaml` and `seq.a3m`. Once the prediction has run, you'll have the option to download a tarfile containing the results. Unarchiving it and looking in the predictions folder, you'll find a `.cif` file. This Crystallographic Information File contains the predicted structure of the below sequence. This can then be uploaded to [Molstar](https://molstar.org/viewer/) to view and interact with the structure.\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "We've covered a lot in this compact example. We've captured the model itself in a convenient Artifact so that we can reference it across workflows and view important metadata. We ran a more traditional workflow in an accelerated way via Actors. We also stood up a persistent app for serving predictions in a flexible and cost effective way using Union Serving. \n",
    "\n",
    "All of this was accomplished programatically via UnionRemote from a Jupyter Notebook! There are many directions to go from here, however this represents an approachable and efficient way of prototyping fairly complex use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
