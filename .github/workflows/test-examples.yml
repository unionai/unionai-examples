name: Test Examples

on:
  # Uncomment to enable automatic testing on push to main
  # push:
  #   branches: [ main ]
  # Uncomment to enable automatic testing on pull requests
  # pull_request:
  #  branches: [ main ]
  # Uncomment to enable daily scheduled testing
  # schedule:
  #   # Run daily at 2 AM UTC
  #   - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: true
        default: 'test'
        type: choice
        options:
          - test-preview
          - test-local
          - test
      filter:
        description: 'Filter pattern for specific tests (optional, mutually exclusive with file)'
        required: false
        type: string
      file:
        description: 'Specific file path to test (optional, mutually exclusive with filter)'
        required: false
        type: string
      python_version:
        description: 'Python version to use'
        required: false
        default: '3.13'
        type: choice
        options:
          - '3.12'
          - '3.13'

jobs:
  test-examples:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.13']

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python ${{ matrix.python-version }}
      run: uv python install ${{ matrix.python-version }}

    - name: Create virtual environment and update Flyte
      run: |
        uv venv --python ${{ matrix.python-version }} .venv
        source .venv/bin/activate
        echo "Virtual environment created with Python ${{ matrix.python-version }}"

    - name: Update to latest Flyte version
      run: |
        source .venv/bin/activate
        make update-flyte

    - name: Clean previous test artifacts
      run: |
        source .venv/bin/activate
        make clean

    - name: Run tests
      env:
        GITHUB_ACTIONS: true
        # Flyte client secret for authentication (referenced in config template)
        FLYTE_CLIENT_SECRET: ${{ secrets.FLYTE_CLIENT_SECRET }}
      run: |
        source .venv/bin/activate
        # Build the make command with optional parameters
        MAKE_CMD="make ${{ github.event.inputs.test_mode || 'test-preview' }}"

        # Add FILE parameter if specified (takes precedence over FILTER)
        if [ -n "${{ github.event.inputs.file }}" ]; then
          MAKE_CMD="$MAKE_CMD FILE=\"${{ github.event.inputs.file }}\""
        elif [ -n "${{ github.event.inputs.filter }}" ]; then
          MAKE_CMD="$MAKE_CMD FILTER=\"${{ github.event.inputs.filter }}\""
        fi

        echo "üöÄ Executing: $MAKE_CMD"
        eval $MAKE_CMD

    - name: Debug test output files
      if: always()
      run: |
        echo "üìÅ Files in test/ directory:"
        find test/ -type f || echo "No test directory found"
        echo "üìÅ Files in test/logs/ directory:"
        find test/logs/ -type f || echo "No test/logs directory found"
        echo "üìÅ Current directory contents:"
        ls -la

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always() && hashFiles('test/logs/*') != ''
      with:
        name: test-results-py${{ matrix.python-version }}
        path: |
          test/logs/
        retention-days: 30

    - name: Upload test reports
      uses: actions/upload-artifact@v4
      if: always() && (hashFiles('test/reports/test_report.html') != '' || hashFiles('test/reports/test_report.json') != '')
      with:
        name: test-reports-py${{ matrix.python-version }}
        path: |
          test/reports/test_report.html
          test/reports/test_report.json
        retention-days: 30

  # Job to combine and publish results from all matrix runs
  publish-results:
    needs: test-examples
    runs-on: ubuntu-latest
    # Only run for manual workflow dispatches, not for PR dry-runs
    if: always() && github.event_name == 'workflow_dispatch' && needs.test-examples.result != 'cancelled'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/

    - name: Debug artifacts
      run: |
        echo "üìÅ Downloaded artifacts structure:"
        find artifacts/ -type f || echo "No artifacts directory found"
        ls -la artifacts/ || echo "Cannot list artifacts directory"

    - name: Combine test results
      run: |
        mkdir -p combined-results/logs
        mkdir -p combined-results/reports
        echo "üìä Combining test results from all Python versions..."

        # Copy all log files from test-results artifacts
        find artifacts/ -path "*/test-results-*" -name "*.log" -exec cp {} combined-results/logs/ \; || echo "No log files found"

        # Copy all report files from test-reports artifacts
        find artifacts/ -path "*/test-reports-*" -name "test_report.json" -exec cp {} combined-results/reports/ \; || echo "No test_report.json files found"
        find artifacts/ -path "*/test-reports-*" -name "test_report.html" -exec cp {} combined-results/reports/ \; || echo "No test_report.html files found"

        echo "üìÅ Combined results structure:"
        echo "Logs directory:"
        ls -la combined-results/logs/ || echo "No logs found"
        echo "Reports directory:"
        ls -la combined-results/reports/ || echo "No reports found"

    - name: Create test summary
      run: |
        echo "# üß™ Test Summary" > combined-results/SUMMARY.md
        echo "" >> combined-results/SUMMARY.md
        echo "## Test Results by Python Version" >> combined-results/SUMMARY.md
        echo "" >> combined-results/SUMMARY.md

        # Analyze JSON reports from the reports directory
        for report in combined-results/reports/test_report.json; do
          if [ -f "$report" ]; then
            echo "- **Report**: $(basename $report)" >> combined-results/SUMMARY.md
            if command -v jq >/dev/null 2>&1; then
              passed_count=$(jq -r '[.[] | select(.status=="passed")] | length' "$report" 2>/dev/null || echo "0")
              failed_count=$(jq -r '[.[] | select(.status=="failed")] | length' "$report" 2>/dev/null || echo "0")
              timeout_count=$(jq -r '[.[] | select(.status=="timeout")] | length' "$report" 2>/dev/null || echo "0")
              total_count=$(jq -r 'length' "$report" 2>/dev/null || echo "0")
              echo "  - **Status**: $passed_count passed, $failed_count failed, $timeout_count timeout" >> combined-results/SUMMARY.md
              echo "  - **Total**: $total_count tests" >> combined-results/SUMMARY.md
            else
              echo "  - **Status**: JSON analysis requires jq (not available)" >> combined-results/SUMMARY.md
            fi
            echo "" >> combined-results/SUMMARY.md
          fi
        done

        # Count log files
        log_count=$(find combined-results/logs/ -name "*.log" | wc -l)
        report_count=$(find combined-results/reports/ -name "*.json" -o -name "*.html" | wc -l)

        echo "## üìã Artifacts Generated" >> combined-results/SUMMARY.md
        echo "" >> combined-results/SUMMARY.md
        echo "- **Log files**: $log_count individual test logs" >> combined-results/SUMMARY.md
        echo "- **Report files**: $report_count JSON/HTML reports" >> combined-results/SUMMARY.md
        echo "- **Structure**:" >> combined-results/SUMMARY.md
        echo "  - \`logs/\` - Individual test execution logs" >> combined-results/SUMMARY.md
        echo "  - \`reports/\` - JSON and HTML formatted test reports" >> combined-results/SUMMARY.md

    - name: Upload combined results
      uses: actions/upload-artifact@v4
      with:
        name: combined-test-results
        path: combined-results/
        retention-days: 90

  # Job to deploy results to GitHub Pages
  deploy-pages:
    needs: publish-results
    runs-on: ubuntu-latest
    # Deploy for manual workflow dispatches that completed (success or failure)
    if: always() && github.event_name == 'workflow_dispatch' && needs.publish-results.result != 'cancelled'

    # Grant GITHUB_TOKEN the permissions required to make a Pages deployment
    permissions:
      pages: write      # to deploy to Pages
      id-token: write   # to verify the deployment originates from an appropriate source

    # Deploy to the github-pages environment
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - name: Download combined results
      uses: actions/download-artifact@v4
      with:
        name: combined-test-results
        path: pages-content/

    - name: Create GitHub Pages content
      run: |
        cd pages-content

        # Copy the HTML report as the main index page
        if [ -f "reports/test_report.html" ]; then
          cp reports/test_report.html index.html
          echo "üìÑ Using test_report.html as main index page"
        else
          echo "‚ö†Ô∏è test_report.html not found, creating fallback index"
          cat > index.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Flyte Examples Test Results</title>
        </head>
        <body>
            <h1>Test Results Not Available</h1>
            <p>No test report was generated.</p>
        </body>
        </html>
        EOF
        fi

        # Also copy the JSON report to the root for easy access
        if [ -f "reports/test_report.json" ]; then
          cp reports/test_report.json test_report.json
          echo "ÔøΩ Copied test_report.json to root"
        fi

        echo "üìÅ Final GitHub Pages structure:"
        find . -type f | sort    - name: Setup Pages
      uses: actions/configure-pages@v4
      with:
        enablement: true

    - name: Upload to GitHub Pages
      uses: actions/upload-pages-artifact@v3
      with:
        path: pages-content/

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4

    - name: Comment deployment URL
      run: |
        echo "üöÄ Test results deployed to GitHub Pages!"
        echo "üìä View results at: ${{ steps.deployment.outputs.page_url }}"