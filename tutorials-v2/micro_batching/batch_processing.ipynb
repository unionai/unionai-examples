{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e665ed",
   "metadata": {},
   "source": [
    "## Solutions Note: Batching Strategies for efficient scale\n",
    "\n",
    "This notebook demonstrates a production-ready pattern for processing millions of items efficiently using Flyte v2's advanced features. You'll learn how to build resilient, scalable workflows that can handle failures gracefully and optimize resource consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41815f6b",
   "metadata": {},
   "source": [
    "### Use Case\n",
    "\n",
    "**The Challenge:** Processing massive datasets (100K to 1M+ items) that require external API calls or long-running operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6df20",
   "metadata": {},
   "source": [
    "**Real-World Examples:**\n",
    "- Web scraping large lists of URLs\n",
    "- Batch inference on millions of data points\n",
    "- Processing documents through external APIs\n",
    "- ETL pipelines with rate-limited services\n",
    "- Data validation against third-party services\n",
    "\n",
    "**The Problem:** When you have so many inputs that you must:\n",
    "1. Split them into batches \n",
    "2. Submit each batch to an external service and wait for completion\n",
    "3. Handle failures without losing progress\n",
    "4. Optimize resource usage across thousands of operations\n",
    "\n",
    "**Why This Matters:** Without proper batching and checkpointing, a single failure in a million-item workflow could force you to restart from scratch, wasting compute resources and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe488b",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce491903",
   "metadata": {},
   "source": [
    "**Our Goals:**\n",
    "1. **Resilience:** Mitigate the impact of batches that take longer or fail\n",
    "2. **Determinism:** Make operations with external API dependencies predictable and resumable\n",
    "3. **Efficiency:** Optimize resource consumption through container reuse and parallel processing\n",
    "4. **Cost Savings:** Minimize wasted compute by checkpointing progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4f31d",
   "metadata": {},
   "source": [
    "## Solution Architecture\n",
    "\n",
    "This example demonstrates a production-ready micro-batching pattern that combines some Union features, including:\n",
    "\n",
    "### 1. Failure transparency with @flyte.trace\n",
    "The `@flyte.trace` decorator creates automatic checkpoints:\n",
    "- **What it does:** Records inputs and outputs of decorated functions\n",
    "- **Why it matters:** If a task fails, it resumes from the last successful checkpoint\n",
    "- **Result:** No re-execution of completed work\n",
    "\n",
    "### 2. Reusable Containers for Efficiency\n",
    "Instead of creating a new container for each task:\n",
    "- **Container pools:** Pre-warmed replicas ready to handle work\n",
    "- **Concurrent processing:** Each replica handles multiple items simultaneously\n",
    "- **Automatic scaling:** Replicas scale between min/max based on workload\n",
    "- **Resource optimization:** Dramatically reduced startup overhead\n",
    "\n",
    "### Key Benefits:\n",
    "- **Automatic checkpointing** at batch and operation boundaries  \n",
    "- **Resume from last successful point** on any failure  \n",
    "- **No wasted compute** - never re-execute completed work  \n",
    "- **Massive parallelism** - process thousands of batches concurrently  \n",
    "- **Cost efficient** - container reuse minimizes cold-start overhead  \n",
    "\n",
    "### Architecture Flow:\n",
    "```\n",
    "1M items â†’ Split into 1,000 batches (1K each)\n",
    "         â†“\n",
    "    Parallel processing across reusable container pool\n",
    "         â†“\n",
    "    Each batch: Submit â†’ Poll â†’ Checkpoint\n",
    "         â†“\n",
    "    Aggregate results from all batches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c9808",
   "metadata": {},
   "source": [
    "### Architecture Diagram\n",
    "\n",
    "![Micro-batching Architecture](./images/micro-batching.png)\n",
    "\n",
    "**Diagram shows:**\n",
    "- Input data split into batches\n",
    "- Reusable container pool \n",
    "- Concurrent processing within each replica \n",
    "- Submit and wait phases with `@flyte.trace` checkpoints\n",
    "- Parallel execution across all batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73fb0b",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66fd748",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Flyte Configuration\n",
    "\n",
    "Configure your connection to the Flyte cluster. This tells Flyte where to run your workflows and how to build container images.\n",
    "\n",
    "**Configuration Options:**\n",
    "- `endpoint`: Your Flyte cluster URL\n",
    "- `org`: Your organization name\n",
    "- `project`: Project to organize workflows\n",
    "- `domain`: Environment (development, staging, production)\n",
    "- `image_builder`: Use \"remote\" to build images on the cluster (no local Docker required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86abb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to your Flyte cluster\n",
    "# Replace these values with your own cluster details\n",
    "\n",
    "import flyte\n",
    "flyte.init(\n",
    "    endpoint=\"https://demo.hosted.unionai.cloud\",  # Your Flyte cluster URL\n",
    "    org=\"demo\",                                     # Your organization\n",
    "    project=\"davide\",                               # Your project name\n",
    "    domain=\"development\",                           # Environment: development/staging/production\n",
    "    image_builder=\"remote\",                         # Build images on cluster (no local Docker needed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bfcf79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio                          # For concurrent async operations\n",
    "from datetime import timedelta          # For time-based configuration\n",
    "from pathlib import Path                # For file path handling\n",
    "from typing import Dict, List           # For type hints\n",
    "\n",
    "import flyte                            # Main Flyte SDK\n",
    "from flyte.remote import Run            # For interacting with remote executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "712bcccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION: Adjust these for your use case\n",
    "# ============================================\n",
    "\n",
    "# Total number of items to process\n",
    "# In production, this could be the size of your dataset\n",
    "NUMBER_OF_INPUTS = 1_000_000  # 1 million items\n",
    "\n",
    "# Size of each batch\n",
    "# Considerations for choosing batch size:\n",
    "# - Larger batches: Fewer tasks, more memory per task\n",
    "# - Smaller batches: More granular checkpointing, better parallelism\n",
    "# - Recommendation: Start with 1000-10000 depending on item complexity\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Example calculations:\n",
    "# 1M items Ã· 1K batch = 1,000 parallel batch tasks\n",
    "# Each batch processes 1K items concurrently within its container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07131466",
   "metadata": {},
   "source": [
    "## Step 2: Define Container Image\n",
    "\n",
    "Create a container image specification with all required dependencies.\n",
    "\n",
    "**Key Dependencies:**\n",
    "- `flyte>=2.0.0b37`: Flyte v2 SDK for workflow orchestration\n",
    "- `unionai-reuse>=0.1.3`: Required for Reusable Containers feature\n",
    "\n",
    "**Note:** You can add any additional packages your tasks need (e.g., `httpx` for API calls, `beautifulsoup4` for web scraping, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62b58a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the container image that will run our tasks\n",
    "# This image will be built once and shared across all task executions\n",
    "image = (\n",
    "    flyte.Image.from_debian_base()  # Start with a lightweight Debian base\n",
    "    .with_pip_packages(\n",
    "        \"flyte>=2.0.0b37\",          # Flyte v2 SDK\n",
    "        \"unionai-reuse>=0.1.3\"      # Required for reusable containers\n",
    "        # Add your own dependencies here\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd678ce",
   "metadata": {},
   "source": [
    "## Step 3: Define Task Environments\n",
    "\n",
    "Task environments encapsulate the runtime configuration for tasks. We'll create one with **Reusable Containers** for efficient batch processing.\n",
    "\n",
    "### What are Reusable Containers?\n",
    "\n",
    "Instead of creating a new Kubernetes Pod for every task execution, Reusable Containers maintain a pool of pre-warmed replicas that can handle multiple tasks sequentially or concurrently.\n",
    "\n",
    "**Benefits:**\n",
    "- **Faster execution:** No container startup overhead (can save 10-60 seconds per task)\n",
    "- **Better resource utilization:** Containers stay warm and handle multiple items\n",
    "- **Cost savings:** Especially significant for tasks with expensive initialization\n",
    "- **Concurrent processing:** Each replica can process multiple items simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6951e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TaskEnvironment with Reusable Containers for batch processing\n",
    "batch_env = flyte.TaskEnvironment(\n",
    "    name=\"batch_processor\",  # Name used for Kubernetes pods: batch_processor-<hash>\n",
    "    \n",
    "    # Resource allocation per replica (per pod)\n",
    "    resources=flyte.Resources(\n",
    "        memory=\"2Gi\",  # Memory per replica\n",
    "        cpu=\"1\"        # CPU cores per replica\n",
    "    ),\n",
    "    \n",
    "    # Reusable container configuration\n",
    "    reusable=flyte.ReusePolicy(\n",
    "        # Number of replica pods to maintain\n",
    "        # (min, max) - scales between these values based on workload\n",
    "        replicas=(3, 10),  # Start with 3, scale up to 10 as needed\n",
    "        \n",
    "        # Concurrency: How many items each replica processes simultaneously\n",
    "        # Higher = more throughput per replica, but more memory usage\n",
    "        concurrency=5,  # Each pod handles 5 concurrent operations\n",
    "        \n",
    "        # How long idle replicas stay alive before being torn down\n",
    "        idle_ttl=timedelta(minutes=5),  # Keep warm for 5 minutes\n",
    "    ),\n",
    "    \n",
    "    # Use the container image we defined earlier\n",
    "    image=image,\n",
    ")\n",
    "\n",
    "# CAPACITY CALCULATION:\n",
    "# With replicas=(3, 10) and concurrency=5:\n",
    "# - Minimum concurrent processing: 3 replicas Ã— 5 concurrency = 15 operations\n",
    "# - Maximum concurrent processing: 10 replicas Ã— 5 concurrency = 50 operations\n",
    "# \n",
    "# For 1,000 batches with these settings:\n",
    "# - Best case: 50 batches processing simultaneously\n",
    "# - Time to process all: ~20 rounds of execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82202bf",
   "metadata": {},
   "source": [
    "### Understanding TaskEnvironment Parameters\n",
    "\n",
    "**name:** \n",
    "- Used as the prefix for Kubernetes pod names\n",
    "- Example: `batch_processor-abc123`\n",
    "\n",
    "**resources:** \n",
    "- Compute resources allocated to *each replica*\n",
    "- Set based on your task's memory and CPU needs\n",
    "- Tip: Monitor actual usage and adjust accordingly\n",
    "\n",
    "**replicas (min, max):**\n",
    "- Flyte autoscales between these values based on workload\n",
    "- More replicas = more parallel processing capacity\n",
    "- Consider your cluster's capacity and quota limits\n",
    "\n",
    "**concurrency:**\n",
    "- Number of async operations each Python process (per pod) handles simultaneously\n",
    "- This is *within* each replica, not across replicas\n",
    "- Higher values increase throughput but require more memory\n",
    "- Best for I/O-bound tasks (API calls, web scraping)\n",
    "- For CPU-bound tasks, keep this lower (1-2)\n",
    "\n",
    "**idle_ttl:**\n",
    "- Time replicas stay alive without active work before shutdown\n",
    "- Longer TTL = faster subsequent executions, higher resource costs\n",
    "- Shorter TTL = lower costs, potential startup delays\n",
    "- Recommendation: 5-15 minutes for typical workloads\n",
    "\n",
    "**image:**\n",
    "- The container image specification with all dependencies\n",
    "- Built once and reused across all task executions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329d18c",
   "metadata": {},
   "source": [
    "### Creating the Orchestrator Environment\n",
    "\n",
    "The orchestrator task coordinates all batch processing but doesn't need container reuse since it only runs once per workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "378ab5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate environment for the orchestrator task\n",
    "orchestrator_env = flyte.TaskEnvironment(\n",
    "    name=\"orchestrator\",\n",
    "    \n",
    "    # depends_on: Use the same image as batch_env (avoids rebuilding)\n",
    "    # Flyte will build batch_env's image first, then reuse it here.\n",
    "    # This is also needed as the orchestrator task calls batch tasks that use batch_env.\n",
    "    depends_on=[batch_env],\n",
    "    \n",
    "    # Orchestrator needs more memory to track all batch executions\n",
    "    # but doesn't need reusable containers (runs once per workflow)\n",
    "    resources=flyte.Resources(\n",
    "        memory=\"4Gi\",  # More memory to manage many parallel batches\n",
    "        cpu=\"1\"        # Single CPU is sufficient for orchestration\n",
    "    ),\n",
    "    \n",
    "    image=image,  # Same image, different resource allocation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d12f5c",
   "metadata": {},
   "source": [
    "### Why Two Environments?\n",
    "\n",
    "**Separation of Concerns:**\n",
    "- **Batch Environment:** Does the heavy lifting (processing items)\n",
    "  - Needs reusable containers for efficiency\n",
    "  - Scales horizontally (many replicas)\n",
    "  - I/O bound operations benefit from concurrency\n",
    "\n",
    "- **Orchestrator Environment:** Coordinates the workflow\n",
    "  - Runs once per workflow execution\n",
    "  - Doesn't need container reuse\n",
    "  - Needs enough memory to track all batches\n",
    "  - CPU bound for coordination logic\n",
    "\n",
    "This separation optimizes both cost and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825a847",
   "metadata": {},
   "source": [
    "## Step 4: Define External Service Interactions\n",
    "\n",
    "These helper functions simulate interactions with external services (APIs, web scraping, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3707e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def submit_to_service(request_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Submit a request to an external service and get a job ID.\n",
    "    \n",
    "    This simulates the \"submit\" phase of a batch job pattern where you:\n",
    "    1. Send data to an external service\n",
    "    2. Receive a job/task ID for tracking\n",
    "    3. Use that ID to poll for completion later\n",
    "    \n",
    "    PRODUCTION IMPLEMENTATION:\n",
    "    Replace this simulation with your actual service call:\n",
    "    \n",
    "    ```python\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(\n",
    "            \"https://your-service.com/api/submit\",\n",
    "            json={\"request_id\": request_id, \"data\": your_data},\n",
    "            timeout=30.0\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"job_id\"]\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        request_id: Unique identifier for this request\n",
    "        \n",
    "    Returns:\n",
    "        job_id: Identifier to track this job's progress\n",
    "    \"\"\"\n",
    "    await asyncio.sleep(0.01)  # Simulate network latency\n",
    "    job_id = f\"job_{request_id}\"\n",
    "    return job_id\n",
    "\n",
    "\n",
    "async def poll_job_status(job_id: str, request_id: int) -> int:\n",
    "    \"\"\"\n",
    "    Poll an external service until a job completes and return results.\n",
    "    \n",
    "    This simulates the \"wait\" phase where you:\n",
    "    1. Repeatedly check if a submitted job has completed\n",
    "    2. Wait between checks to avoid overwhelming the service\n",
    "    3. Return the final result when ready\n",
    "    \n",
    "    PRODUCTION IMPLEMENTATION:\n",
    "    Replace this simulation with your actual polling logic:\n",
    "    \n",
    "    ```python\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        max_attempts = 60  # 5 minutes with 5-second intervals\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            response = await client.get(\n",
    "                f\"https://your-service.com/api/status/{job_id}\",\n",
    "                timeout=10.0\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            status = response.json()\n",
    "            \n",
    "            if status[\"state\"] == \"completed\":\n",
    "                return status[\"result\"]\n",
    "            elif status[\"state\"] == \"failed\":\n",
    "                raise Exception(f\"Job {job_id} failed: {status['error']}\")\n",
    "            \n",
    "            # Wait before next poll\n",
    "            await asyncio.sleep(5)\n",
    "        \n",
    "        raise TimeoutError(f\"Job {job_id} did not complete in time\")\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        job_id: The job identifier from submit_to_service\n",
    "        request_id: Original request ID for logging/tracking\n",
    "        \n",
    "    Returns:\n",
    "        result: The processed result from the external service\n",
    "    \"\"\"\n",
    "    await asyncio.sleep(0.05)  # Simulate polling + processing time\n",
    "    return request_id * 2  # Dummy result\n",
    "\n",
    "# IMPORTANT NOTES:\n",
    "# 1. Both functions are async - they don't block while waiting\n",
    "# 2. Add logging for debugging and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c0f9b",
   "metadata": {},
   "source": [
    "## Step 5: Implement the Batch Processing Task\n",
    "\n",
    "This is the heart of the pattern. The `process_batch` task processes a batch of items with automatic checkpointing using `@flyte.trace`.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Two-Phase Processing:**\n",
    "1. **Submit Phase:** Send all items to external service concurrently\n",
    "2. **Wait Phase:** Poll for completion of all submitted jobs\n",
    "\n",
    "**Why @flyte.trace?**\n",
    "- Creates checkpoints at phase boundaries\n",
    "- If the task fails during wait phase, it resumes from there (doesn't re-submit)\n",
    "- Enables forward recovery without re-execution\n",
    "\n",
    "**Concurrency Pattern:**\n",
    "- Uses `asyncio.gather()` to process all items in a batch simultaneously\n",
    "- `return_exceptions=True` prevents one failure from stopping the batch\n",
    "- Each phase completes fully before moving to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d897cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@batch_env.task  # This task runs in the reusable container pool\n",
    "async def process_batch(batch_start: int, batch_end: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Process a single batch of items with checkpointed phases.\n",
    "    \n",
    "    This function demonstrates the core micro-batching pattern with:\n",
    "    1. Two-phase processing (submit â†’ wait)\n",
    "    2. Automatic checkpointing via @flyte.trace\n",
    "    3. Error handling without stopping the entire batch\n",
    "    4. Concurrent processing within the batch\n",
    "    \n",
    "    Args:\n",
    "        batch_start: Starting index for this batch (inclusive)\n",
    "        batch_end: Ending index for this batch (exclusive)\n",
    "        \n",
    "    Returns:\n",
    "        List of processed results (or -1 for failed items)\n",
    "        \n",
    "    Example:\n",
    "        process_batch(0, 1000) processes items 0-999\n",
    "        process_batch(1000, 2000) processes items 1000-1999\n",
    "    \"\"\"\n",
    "\n",
    "    # ========================================\n",
    "    # PHASE 1: SUBMIT ALL ITEMS TO SERVICE\n",
    "    # ========================================\n",
    "    @flyte.trace  # Creates a checkpoint after this phase completes\n",
    "    async def submit_phase(items: List[int]) -> Dict[int, str]:\n",
    "        \"\"\"\n",
    "        Submit all items concurrently and collect job IDs.\n",
    "        \n",
    "        This function:\n",
    "        1. Launches submit_to_service() for ALL items simultaneously\n",
    "        2. Waits for all submissions to complete with asyncio.gather()\n",
    "        3. Handles errors gracefully (return_exceptions=True)\n",
    "        4. Maps each request_id to its job_id (or None if failed)\n",
    "        \n",
    "        Why @flyte.trace here:\n",
    "        - If this phase succeeds but wait_phase fails, we don't re-submit\n",
    "        - Checkpointed data includes all job_ids for the wait phase\n",
    "        - Forward recovery from exact failure point\n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "        job_ids = await asyncio.gather(\n",
    "            *(submit_to_service(request_id=x) for x in items),\n",
    "            return_exceptions=True  # Don't stop on individual failures\n",
    "        )\n",
    "        \n",
    "        # Map request IDs to job IDs (or None for failures)\n",
    "        job_mapping = {}\n",
    "        for request_id, job_id in zip(items, job_ids):\n",
    "            if isinstance(job_id, Exception):\n",
    "                print(f\"[ERROR] Submit failed for {request_id}: {job_id}\")\n",
    "                job_mapping[request_id] = None  # Mark as failed\n",
    "            else:\n",
    "                job_mapping[request_id] = job_id\n",
    "        \n",
    "        return job_mapping\n",
    "\n",
    "    # ========================================\n",
    "    # PHASE 2: WAIT FOR ALL JOBS TO COMPLETE\n",
    "    # ========================================\n",
    "    @flyte.trace  # Creates another checkpoint after this phase completes\n",
    "    async def wait_phase(job_mapping: Dict[int, str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Poll all submitted jobs until completion.\n",
    "        \n",
    "        This function:\n",
    "        1. Takes the checkpointed job_mapping from submit_phase\n",
    "        2. Polls all jobs concurrently\n",
    "        3. Handles polling errors gracefully\n",
    "        4. Returns final results\n",
    "        \n",
    "        WHY @flyte.trace HERE:\n",
    "        - If polling fails partway through, we resume with cached job_mapping\n",
    "        - Don't re-submit jobs that were already submitted\n",
    "        - Each successful poll is checkpointed\n",
    "        \n",
    "        ERROR HANDLING:\n",
    "        - Jobs that failed in submit_phase (None) are skipped\n",
    "        - Polling failures are caught and marked as -1\n",
    "        - The batch continues even if some items fail\n",
    "        \"\"\"\n",
    "        # Poll ALL jobs concurrently\n",
    "        results = await asyncio.gather(\n",
    "            *(\n",
    "                poll_job_status(job_id=job_id, request_id=request_id)\n",
    "                if job_id is not None  # Only poll successfully submitted jobs\n",
    "                else asyncio.sleep(0)   # Skip failed submissions\n",
    "                for request_id, job_id in job_mapping.items()\n",
    "            ),\n",
    "            return_exceptions=True  # Don't stop on individual failures\n",
    "        )\n",
    "        \n",
    "        # Process results and handle errors\n",
    "        processed_results = []\n",
    "        for request_id, result in zip(job_mapping.keys(), results):\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"[ERROR] Wait failed for {request_id}: {result}\")\n",
    "                processed_results.append(-1)  # Mark as failed\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    # ========================================\n",
    "    # EXECUTE BOTH PHASES SEQUENTIALLY\n",
    "    # ========================================\n",
    "    # Create the list of items for this batch\n",
    "    items = list(range(batch_start, batch_end))\n",
    "    \n",
    "    # Phase 1: Submit all items and get job IDs (checkpointed)\n",
    "    job_mapping = await submit_phase(items)\n",
    "    \n",
    "    # Phase 2: Wait for all jobs to complete (checkpointed)\n",
    "    results = await wait_phase(job_mapping)\n",
    "    \n",
    "    # Log batch completion stats\n",
    "    successful = len([r for r in results if r != -1])\n",
    "    print(f\"Batch {batch_start}-{batch_end}: {successful}/{len(results)} successful\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ========================================\n",
    "# CHECKPOINT & RECOVERY BEHAVIOR\n",
    "# ========================================\n",
    "# \n",
    "# Scenario 1: Task fails during submit_phase\n",
    "# â†’ Retries resume from last checkpoint\n",
    "# \n",
    "# Scenario 2: Task fails after submit_phase completes\n",
    "# â†’ Resumes directly to wait_phase with cached job_mapping\n",
    "# â†’ No re-submissions!\n",
    "# \n",
    "# Scenario 3: Task fails during wait_phase\n",
    "# â†’ Resumes wait_phase with cached job_mapping\n",
    "# â†’ Already-polled jobs are not polled again (Flyte makes operations idempotent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491fc84",
   "metadata": {},
   "source": [
    "### Understanding @flyte.trace\n",
    "\n",
    "**Why use it for both phases:**\n",
    "- Submit phase checkpoint = \"These jobs were submitted successfully\"\n",
    "- Wait phase checkpoint = \"These results were retrieved successfully\"\n",
    "- Without it: A failure in submit or wait phase would re-submit or re-poll everything\n",
    "\n",
    "**Best Practices:**\n",
    "- Use `@flyte.trace` for non-deterministic operations (API calls, random operations)\n",
    "- Don't use it for pure, deterministic functions (unnecessary overhead)\n",
    "- Ensure traced functions are idempotent when possible\n",
    "- Keep traced function signatures simple (serializable inputs/outputs)\n",
    "\n",
    "See the [Traces](https://www.union.ai/docs/v2/byoc/user-guide/task-programming/traces/) docs for more details on how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cda86b",
   "metadata": {},
   "source": [
    "## Step 6: Implement the Orchestrator Workflow\n",
    "\n",
    "The orchestrator is the top-level task that:\n",
    "1. Splits the total workload into batches\n",
    "2. Launches all batches in parallel\n",
    "3. Aggregates results from all batches\n",
    "4. Reports overall statistics\n",
    "\n",
    "**This is where the magic happens:** All batches run concurrently, limited only by your reusable container pool configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1523d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "@orchestrator_env.task  # Runs in the orchestrator environment (no reuse)\n",
    "async def microbatch_workflow(\n",
    "    total_items: int = NUMBER_OF_INPUTS,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Main task orchestrating the entire micro-batching process.\n",
    "    \n",
    "    This task:\n",
    "    1. Calculates optimal batch distribution\n",
    "    2. Launches all batch tasks in parallel\n",
    "    3. Aggregates results from completed batches\n",
    "    4. Provides comprehensive execution statistics\n",
    "    \n",
    "    Args:\n",
    "        total_items: Total number of items to process (default: 1M)\n",
    "        batch_size: Number of items per batch (default: 1K)\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated results from all batches (list of processed values)\n",
    "        \n",
    "    Execution Flow:\n",
    "        1M items â†’ 1,000 batches â†’ Parallel execution â†’ Aggregated results\n",
    "        \n",
    "    Resource Usage:\n",
    "        - This task: 4Gi memory, 1 CPU (orchestration only)\n",
    "        - Each batch task: 2Gi memory, 1 CPU (from batch_env)\n",
    "        - Reusable containers handle actual processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 1: CALCULATE BATCH DISTRIBUTION\n",
    "    # ========================================\n",
    "    # Split total items into batch ranges: [(0, 1000), (1000, 2000), ...]\n",
    "    batches = [\n",
    "        (start, min(start + batch_size, total_items))\n",
    "        for start in range(0, total_items, batch_size)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing {total_items:,} items in {len(batches):,} batches of size {batch_size:,}\")\n",
    "    print(f\"Expected parallelism: {batch_env.reusable.replicas[0]}-{batch_env.reusable.replicas[1]} replicas\")\n",
    "    print(f\"Concurrency per replica: {batch_env.reusable.concurrency}\")\n",
    "    print(f\"Max simultaneous batches: {batch_env.reusable.replicas[1] * batch_env.reusable.concurrency}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 2: LAUNCH ALL BATCHES IN PARALLEL\n",
    "    # ========================================\n",
    "    # This is the key to massive parallelism:\n",
    "    # - Creates as many async tasks as concurrent operations your API supports\n",
    "    # - All execute concurrently within container pool limits\n",
    "    # - Reusable containers handle the workload efficiently\n",
    "    # - return_exceptions=True prevents one batch failure from stopping all\n",
    "    \n",
    "    print(f\"\\n Launching {len(batches):,} parallel batch tasks...\")\n",
    "    \n",
    "    # Rate limiter to control API throughput\n",
    "    max_concurrent_batches = 10  # Adjust based on API rate limits\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_batches)\n",
    "    \n",
    "    async def rate_limited_batch(start: int, end: int):\n",
    "        \"\"\"Wrapper to enforce rate limiting on batch processing.\"\"\"\n",
    "        async with semaphore:\n",
    "            return await process_batch(batch_start=start, batch_end=end)\n",
    "    \n",
    "    batch_results = await asyncio.gather(\n",
    "        *(rate_limited_batch(start, end) for start, end in batches),\n",
    "        return_exceptions=True  # Isolated failure handling per batch\n",
    "    )\n",
    "    # ========================================\n",
    "    # STEP 3: AGGREGATE RESULTS & STATISTICS\n",
    "    # ========================================\n",
    "    all_results = []\n",
    "    failed_batches = 0\n",
    "    failed_items = 0\n",
    "    \n",
    "    for i, batch_result in enumerate(batch_results):\n",
    "        if isinstance(batch_result, Exception):\n",
    "            # Entire batch failed (task-level failure)\n",
    "            print(f\"[ERROR] Batch {i} failed completely: {batch_result}\")\n",
    "            failed_batches += 1\n",
    "        else:\n",
    "            # Batch completed, but individual items may have failed\n",
    "            all_results.extend(batch_result)\n",
    "            failed_items += len([r for r in batch_result if r == -1])\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    success_count = len([r for r in all_results if r != -1])\n",
    "    total_processed = len(all_results)\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 4: REPORT EXECUTION SUMMARY\n",
    "    # ========================================\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\" Execution summary\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Total items requested:    {total_items:,}\")\n",
    "    print(f\"Total batches:            {len(batches):,}\")\n",
    "    print(f\"Batch size:               {batch_size:,}\")\n",
    "    print(f\"\")\n",
    "    print(f\" Successful items:       {success_count:,}\")\n",
    "    print(f\" Failed items:           {failed_items:,}\")\n",
    "    print(f\" Failed batches:         {failed_batches}\")\n",
    "    print(f\"\")\n",
    "    print(f\" Success rate:           {success_count / total_items * 100:.2f}%\")\n",
    "    print(f\" Items processed:        {total_processed:,} / {total_items:,}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# ========================================\n",
    "# EXECUTION BEHAVIOR & OPTIMIZATION\n",
    "# ========================================\n",
    "#\n",
    "# Parallel Execution Pattern:\n",
    "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "# â”‚ Orchestrator Task (1 pod, 4Gi, 1 CPU)         â”‚\n",
    "# â”‚                                                 â”‚\n",
    "# â”‚ Launches 1,000 process_batch() invocations     â”‚\n",
    "# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#                   â”‚\n",
    "#           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "#           â–¼                â–¼\n",
    "#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "#   â”‚ Replica 1    â”‚  â”‚ Replica 2    â”‚  ... up to 10 replicas\n",
    "#   â”‚ 2Gi, 1 CPU   â”‚  â”‚ 2Gi, 1 CPU   â”‚\n",
    "#   â”‚              â”‚  â”‚              â”‚\n",
    "#   â”‚ Concurrency: â”‚  â”‚ Concurrency: â”‚\n",
    "#   â”‚ 5 batches    â”‚  â”‚ 5 batches    â”‚\n",
    "#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#\n",
    "# With 10 replicas Ã— 5 concurrency = 50 batches processing simultaneously\n",
    "# Time to complete 1,000 batches â‰ˆ 1,000 / 50 = 20 waves\n",
    "#\n",
    "# Optimization Tips:\n",
    "# 1. Increase replicas for more parallelism (if cluster allows)\n",
    "# 2. Adjust concurrency based on task I/O vs CPU profile\n",
    "# 3. Tune batch_size to balance granularity vs overhead\n",
    "# 4. Monitor actual execution to find bottlenecks\n",
    "# 5. Use Flyte UI to visualize execution patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fab3",
   "metadata": {},
   "source": [
    "## Step 7: Execute the Workflow\n",
    "\n",
    "Now let's run the entire workflow remotely on your Union cluster.\n",
    "\n",
    "**Execution Options:**\n",
    "- **Remote execution** (shown below): Runs on the Union cluster\n",
    "- **Local execution**: Use `flyte.with_runcontext(mode=\"local\").run()` for testing\n",
    "\n",
    "**What happens during execution:**\n",
    "1. Flyte builds the container image (if needed)\n",
    "2. Creates the orchestrator pod\n",
    "3. Orchestrator calculates batches and launches batch tasks\n",
    "4. Reusable container pool starts spinning up (min: 3 replicas in this example)\n",
    "5. Batches are distributed across available replicas\n",
    "6. Pool scales up to max replicas (10 in this example) as needed\n",
    "7. Results are aggregated and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b80acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " STARTING MICRO-BATCHING WORKFLOW\n",
      "============================================================\n",
      "Total items to process: 1,000,000\n",
      "Batch size: 1,000\n",
      "Expected batches: 1,000\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">22:28:45.188135 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> _deploy.py:396 -  Built Image for environment orchestrator, image:                                                                     \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         356633062068.dkr.ecr.us-east-2.amazonaws.com/union/demo:flyte-e1efc5547b793bdd23027db671363154                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m22:28:45.188135\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m _deploy.py:396 -  Built Image for environment orchestrator, image:                                                                     \n",
       "\u001b[2;36m                \u001b[0m         356633062068.dkr.ecr.us-east-2.amazonaws.com/union/demo:flyte-e1efc5547b793bdd23027db671363154                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">22:28:45.192140 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> _deploy.py:396 -  Built Image for environment batch_processor, image:                                                                  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         356633062068.dkr.ecr.us-east-2.amazonaws.com/union/demo:flyte-e1efc5547b793bdd23027db671363154                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m22:28:45.192140\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m _deploy.py:396 -  Built Image for environment batch_processor, image:                                                                  \n",
       "\u001b[2;36m                \u001b[0m         356633062068.dkr.ecr.us-east-2.amazonaws.com/union/demo:flyte-e1efc5547b793bdd23027db671363154                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " EXECUTION STARTED\n",
      "============================================================\n",
      "ğŸ”— Execution URL: https://demo.hosted.unionai.cloud/v2/domain/development/project/davide/runs/rkpzk44ggqgfct9pcm5z\n",
      "\n",
      "ğŸ’¡ Visit the URL above to:\n",
      "   â€¢ View the execution graph and task timeline\n",
      "   â€¢ Monitor progress in real-time\n",
      "   â€¢ See trace checkpoints in action\n",
      "   â€¢ Inspect logs for each batch\n",
      "   â€¢ Analyze resource utilization\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\" STARTING MICRO-BATCHING WORKFLOW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total items to process: {NUMBER_OF_INPUTS:,}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE:,}\")\n",
    "    print(f\"Expected batches: {NUMBER_OF_INPUTS // BATCH_SIZE:,}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Launch the workflow remotely (runs on Flyte cluster)\n",
    "    # The 'await' is needed because flyte.run.aio() is async\n",
    "    r = await flyte.run.aio(microbatch_workflow)\n",
    "    \n",
    "    # Print execution details\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\" EXECUTION STARTED\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    # print(f\"Run name: {r.name}\")  # Internal run identifier\n",
    "    print(f\"ğŸ”— Execution URL: {r.url}\")\n",
    "    print(f\"\\nğŸ’¡ Visit the URL above to:\")\n",
    "    print(f\"   â€¢ View the execution graph and task timeline\")\n",
    "    print(f\"   â€¢ Monitor progress in real-time\")\n",
    "    print(f\"   â€¢ See trace checkpoints in action\")\n",
    "    print(f\"   â€¢ Inspect logs for each batch\")\n",
    "    print(f\"   â€¢ Analyze resource utilization\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "\n",
    "# ========================================\n",
    "# MONITORING AND DEBUGGING TIPS\n",
    "# ========================================\n",
    "#\n",
    "# 1. View Execution in UI:\n",
    "#    - Click the execution URL printed above\n",
    "#    - See visual graph of all batch tasks\n",
    "#    - Monitor which batches are running/completed/failed\n",
    "#\n",
    "# 2. Check Logs:\n",
    "#    - Click on individual batch tasks in the graph\n",
    "#    - View stdout/stderr for debugging\n",
    "#    - See checkpoint/recovery messages\n",
    "#\n",
    "# 3. Resource Utilization:\n",
    "#    - Navigate to Resources tab in UI\n",
    "#    - Monitor CPU/memory usage per task\n",
    "#    - Identify bottlenecks or over-provisioning\n",
    "#\n",
    "# 4. Trace Visualization:\n",
    "#    - Expand batch tasks to see trace checkpoints\n",
    "#    - Verify submit_phase and wait_phase separately\n",
    "#    - Understand recovery points on failures\n",
    "#\n",
    "# 5. Performance Analysis:\n",
    "#    - Check task durations in timeline view\n",
    "#    - Identify slow batches or stragglers\n",
    "#    - Optimize batch_size or concurrency based on results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e23657",
   "metadata": {},
   "source": [
    "On execution, this is what this example looks like at the Kubernetes level:\n",
    "\n",
    "![](./images/reusable-containers-k8s.png)\n",
    "\n",
    "This is, 10 replicas (as defined in the `TaskEnvironment`) and the driver Pod that runs the parent task (`a0`). [Learn more about the parent task](https://www.union.ai/docs/v2/byoc/user-guide/considerations/#driver-pod-requirements)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40313de0",
   "metadata": {},
   "source": [
    "## Batch Size Selection\n",
    "\n",
    "**Finding the optimal batch size:**\n",
    "- **Too small:** More overhead from task management, less efficient\n",
    "- **Too large:** Longer recovery time on failures, higher memory usage\n",
    "\n",
    "\n",
    "**Factors to consider:**\n",
    "- Item processing time (longer = larger batches)\n",
    "- Memory consumption per item (higher = smaller batches)\n",
    "- Failure tolerance (critical = smaller batches for faster recovery)\n",
    "- Total workload size (larger total = can use larger batches)\n",
    "\n",
    "Read the [Optimization strategies](https://www.union.ai/docs/v2/byoc/user-guide/run-scaling/scale-your-workflows/#2-batch-workloads-to-reduce-overhead) page to understand the overheads associated with an execution and how to choose the appropiate batch size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b17874",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a production-ready micro-batching pattern for Flyte v2 that combines:\n",
    "\n",
    "1. **Reusable Containers** for efficiency\n",
    "2. **@flyte.trace** for checkpointing and recovery\n",
    "3. **Massive parallelism** via async/await\n",
    "4. **Robust error handling** for resilience\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Use `@flyte.trace` for non-deterministic operations\n",
    "- Monitor resource usage and optimize incrementally\n",
    "- Choose the right pattern for your specific use case\n",
    "\n",
    "**Next Steps:**\n",
    "- Adapt this pattern to your specific use case\n",
    "- Replace mock functions with real API calls\n",
    "- Test with your actual dataset\n",
    "- Monitor and optimize based on production metrics\n",
    "\n",
    "For questions or issues, refer to [Flyte v2 documentation](https://www.union.ai/docs/v2/byoc/user-guide/flyte-2/) or reach out to [Union.ai support](https://www.union.ai/consultation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unionai-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
